{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vHrqPAHVut1"
      },
      "source": [
        "**EVALUATION OF FINETUNED MODELS**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBgTGEXZ0KXp",
        "outputId": "b7ff4c53-81a4-4300-bb4f-659e2e112d0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from copy import copy\n",
        "import tensorflow as tf\n",
        "import sklearn.metrics\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics._plot.confusion_matrix import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gYgZPqLV0Ck"
      },
      "source": [
        "# Load and read and preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WSawq_NV25Dr"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/results/'\n",
        "predicted_model_folder = 'baseline_LaBSE/'\n",
        "path_to_predicted_file = f'{predicted_model_folder}predictionsbaseline_labse_new.csv'\n",
        "model_path_folder = 'my_model_v1baseline_labse_new'\n",
        "\n",
        "# predicted_model_folder = 'baseline_distilBERT/'\n",
        "# path_to_predicted_file = f'{predicted_model_folder}predictionsbaseline_distilbert.csv'\n",
        "# model_path_folder = 'my_model_v1baseline_distilbert'\n",
        "\n",
        "# ##paraphrasing\n",
        "# predicted_model_folder = 'labse_oversampling_3classes/'\n",
        "# path_to_predicted_file = f'{predicted_model_folder}predictionslabse_oversampling_3classes.csv'\n",
        "# model_path_folder = 'my_model_v1labse_oversampling_3classes'\n",
        "\n",
        "# ##zsl and fsl\n",
        "# predicted_model_folder = 'distilbert_augmented24_gpt4/'\n",
        "# path_to_predicted_file = f'{predicted_model_folder}predictionsdistilbert_augmented11.csv'\n",
        "# model_path_folder = 'my_model_v1distilbert_augmented11'\n",
        "\n",
        "# ##BT\n",
        "# predicted_model_folder = 'labse_augmented35_gpt4/'\n",
        "# path_to_predicted_file = f'{predicted_model_folder}predictionslabse_augmented35.csv'\n",
        "# model_path_folder = 'my_model_v1labse_augmented35'\n",
        "\n",
        "# ##oversampling\n",
        "# predicted_model_folder = 'distilbert_oversampling_5classes/'\n",
        "# path_to_predicted_file = f'{predicted_model_folder}predictionsdistilbert_dummy_copies_5classes.csv'\n",
        "# model_path_folder = 'my_model_v1distilbert_dummy_copies_5classes'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WHosRMi45pre"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# files.download(f'{path}test.tsv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "99AB-31DBk-L",
        "outputId": "3490fa52-6362-410e-bf8a-d2d9516fd9a3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/results/test.tsv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-736706534.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{path}test.tsv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/results/test.tsv'"
          ]
        }
      ],
      "source": [
        "test_df = pd.read_csv(f'{path}test.tsv', sep='\\t', header=None)\n",
        "print(len(test_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lb5gT9AKydfv"
      },
      "outputs": [],
      "source": [
        "##Load saved model\n",
        "# loaded_model = load_model(f'{path}/my_model_v1baseline.h5', compile = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lhr0Aw8QKd2d"
      },
      "outputs": [],
      "source": [
        "# Load file 'emotions.txt'\n",
        "with open(f'{path}emotions.txt','r') as file:\n",
        "  emotions = [line.strip() for line in file.readlines()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPttc-HH3bTA"
      },
      "outputs": [],
      "source": [
        "# # Load the file 'predictionsbaseline.txt'\n",
        "# with open(f'{path}/predictions.txt', 'r') as file:\n",
        "#     prd = file.read()\n",
        "# lists = re.findall(r'\\[(.*?)\\]', prd, re.DOTALL)\n",
        "\n",
        "# predictions = []\n",
        "\n",
        "# for lst in lists:\n",
        "#     float_elements = [float(e) for e in lst.split() if e]\n",
        "#     predictions.append(float_elements)\n",
        "\n",
        "# predictions_array = np.array(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bi9C_wd2yMr"
      },
      "outputs": [],
      "source": [
        "## Load prediction arrays\n",
        "file_path = f'{path}/{path_to_predicted_file}'\n",
        "\n",
        "df = pd.read_csv(file_path, header=None)\n",
        "\n",
        "# Convert the DataFrame to a NumPy array\n",
        "predictions_array = df.to_numpy()\n",
        "predictions_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRZ75JF0DK2c"
      },
      "outputs": [],
      "source": [
        "##check if multi-label\n",
        "multi_label_rows = [row for row in predictions_array if np.sum(row) > 1]\n",
        "\n",
        "if multi_label_rows:\n",
        "    print(\"The predictions are multi-label for the following rows:\")\n",
        "    for row in multi_label_rows:\n",
        "        print(row)\n",
        "        break\n",
        "else:\n",
        "    print(\"The predictions are single-label.\")\n",
        "\n",
        "print(f'Number of elements that were predicted as multi-label: {len(multi_label_rows)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnbI5pAJUtv3"
      },
      "outputs": [],
      "source": [
        "## Let's recreate test_labels (ground truth) for evaluating results\n",
        "\n",
        "# Transform the labels\n",
        "def convert_to_vec(el,num=28):\n",
        "  lst = [0.0 for _ in range(num)]\n",
        "  for num in el:\n",
        "    lst[int(num)] =1\n",
        "  return lst\n",
        "\n",
        "test_X_df, test_labels_df = test_df[0], test_df[1]\n",
        "\n",
        "test_labs = []\n",
        "for el in test_labels_df.values:\n",
        "    n_l = []\n",
        "    l = re.split(',', el)\n",
        "    for e in l:\n",
        "        n_l.append(np.float64(e))\n",
        "    test_labs.append(copy(convert_to_vec(n_l)))\n",
        "test_labs = np.asarray(test_labs).astype(np.float32)\n",
        "\n",
        "##check prediction equal test labs (ground truth)\n",
        "print(\"Is PREDICTED length equal to GROUND TRUTH? ----->\",len(predictions_array) == len(test_labs))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3P8Q1ZrX6l89"
      },
      "source": [
        "#Make prediction one more time to get prediction probabilites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVg_Surd-vXe"
      },
      "outputs": [],
      "source": [
        "!pip install ktrain transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3hZI01tILV2"
      },
      "outputs": [],
      "source": [
        "import ktrain\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from ktrain import text as ktext\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EfQC2SvMRvm"
      },
      "outputs": [],
      "source": [
        "test_X_df,test_labels_df = test_df[0],test_df[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWVA9QweMCfe"
      },
      "outputs": [],
      "source": [
        "def convert_to_vec(el,num=28):\n",
        "  lst = [0.0 for _ in range(num)]\n",
        "  for num in el:\n",
        "    lst[int(num)] =1\n",
        "  return lst\n",
        "\n",
        "\n",
        "test_labs = []\n",
        "for el in test_labels_df.values:\n",
        "  n_l = []\n",
        "  l = re.split(',',el)\n",
        "  for e in l:\n",
        "    n_l.append(np.float64(e))\n",
        "  test_labs.append(copy(convert_to_vec(n_l)))\n",
        "test_labs = np.asarray(test_labs).astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9o6zX6GwaBK"
      },
      "outputs": [],
      "source": [
        "# test_labs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##I aproach"
      ],
      "metadata": {
        "id": "Ze-cTEL6aBQT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfAMj4-GDR_r"
      },
      "outputs": [],
      "source": [
        "#load model\n",
        "model_path = f'{path}{predicted_model_folder}{model_path_folder}'\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxQo4eOsQR6b"
      },
      "outputs": [],
      "source": [
        "##preprocessing test data\n",
        "# MODEL_NAME = \"distilbert-base-uncased\"\n",
        "MODEL_NAME = \"sentence-transformers/LaBSE\"\n",
        "t = ktext.Transformer(MODEL_NAME, maxlen=124, class_names=emotions)\n",
        "\n",
        "test = t.preprocess_test(list(test_X_df), test_labs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VF7mk-ucbzZM"
      },
      "outputs": [],
      "source": [
        "input_ids = [sample[0][0][0] for sample in test]\n",
        "attention_mask = [sample[0][0][1] for sample in test]\n",
        "labels = [sample[1][0] for sample in test]\n",
        "\n",
        "input_ids = tf.convert_to_tensor(input_ids)\n",
        "attention_mask = tf.convert_to_tensor(attention_mask)\n",
        "labels = tf.convert_to_tensor(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3B4k4TKgLIU"
      },
      "outputs": [],
      "source": [
        "prediction_inputs = {\n",
        "    'input_ids': input_ids,\n",
        "    'attention_mask': attention_mask\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6OazY1SQ2zS"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(prediction_inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHYLF-WViZSK"
      },
      "outputs": [],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cez30c8nitIg"
      },
      "outputs": [],
      "source": [
        "probabilities = tf.nn.softmax(predictions.logits, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WRmkggKix5g"
      },
      "outputs": [],
      "source": [
        "probabilities[5426]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPpO4pWGjFLG"
      },
      "outputs": [],
      "source": [
        "# #evaluation\n",
        "# from sklearn.metrics import f1_score\n",
        "\n",
        "# Convert probabilities to binary predictions (you may choose a threshold other than 0.5)\n",
        "threshold = 0.5\n",
        "\n",
        "binary_predictions = tf.cast(probabilities > threshold, tf.int32)\n",
        "\n",
        "# Calculate F1 Score\n",
        "f1 = f1_score(labels, binary_predictions, average='macro')  # or 'macro' based on your requirement\n",
        "print(f\"F1 Score: {f1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##II aproach with preproc"
      ],
      "metadata": {
        "id": "zlmkdv12XVBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model_path_folder2='my_model_v1_with_preprocesorlabse_augmented3'\n",
        "# new_model_path = f'{path}{predicted_model_folder}{model_path_folder2}'\n",
        "\n",
        "# import os\n",
        "# for file in os.listdir(new_model_path):\n",
        "#     print(file)\n",
        "\n"
      ],
      "metadata": {
        "id": "dRlm1ifLXUJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #load model and preproc\n",
        "# predictor = ktrain.load_predictor(new_model_path)"
      ],
      "metadata": {
        "id": "89DG0lUPcEBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prediction = predictor.predict(list(test_X_df))"
      ],
      "metadata": {
        "id": "u8rvKdlcZloC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8Btvk5vVgfB"
      },
      "source": [
        "# Evaluate quality of model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txXxEomEgrxf"
      },
      "outputs": [],
      "source": [
        "# Overall Accuracy\n",
        "overall_accuracy = accuracy_score(test_labs, binary_predictions)\n",
        "print(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n",
        "\n",
        "# Overall Macro and Micro F1 Scores\n",
        "macro_f1 = f1_score(test_labs, binary_predictions, average='macro')\n",
        "micro_f1 = f1_score(test_labs, binary_predictions, average='micro')\n",
        "print(f\"Macro F1 Score: {macro_f1:.4f}\")\n",
        "print(f\"Micro F1 Score: {micro_f1:.4f}\")\n",
        "\n",
        "# ##Precision, Recall, and F1-score\n",
        "\n",
        "precision_macro = precision_score(test_labs, binary_predictions, average='macro')\n",
        "recall_macro = recall_score(test_labs, binary_predictions, average='macro')\n",
        "f1_macro= f1_score(test_labs, binary_predictions, average='macro')\n",
        "print(f\"Macro Precision: {precision_macro:.4f}\")\n",
        "print(f\"Macro Recall: {recall_macro:.4f}\")\n",
        "print(f\"Macro F1 Score: {f1_macro:.4f}\")\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "precision_micro = precision_score(test_labs, binary_predictions, average='micro')\n",
        "recall_micro = recall_score(test_labs, binary_predictions, average='micro')\n",
        "f1_micro = f1_score(test_labs, binary_predictions, average='micro')\n",
        "print(f\"Micro Precision: {precision_micro:.4f}\")\n",
        "print(f\"Micro Recall: {recall_micro:.4f}\")\n",
        "print(f\"Micro F1 Score: {f1_micro:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TziB6Cggt0D"
      },
      "outputs": [],
      "source": [
        "# ##Hamming Loss\n",
        "from sklearn.metrics import hamming_loss\n",
        "\n",
        "hamming = hamming_loss(test_labs, binary_predictions)\n",
        "print(f\"Hamming Loss: {hamming:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AVU8HFtguNA"
      },
      "outputs": [],
      "source": [
        "##Confusion Matrix\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "\n",
        "cm = multilabel_confusion_matrix(test_labs, binary_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfAi2tGHAEri"
      },
      "outputs": [],
      "source": [
        "#get tn,fp,fn,tp for each class\n",
        "tn_list = []\n",
        "fp_list = []\n",
        "fn_list = []\n",
        "tp_list = []\n",
        "\n",
        "for i in range(28):\n",
        "    tn, fp, fn, tp = cm[i].ravel()\n",
        "    tn_list.append(tn)\n",
        "    fp_list.append(fp)\n",
        "    fn_list.append(fn)\n",
        "    tp_list.append(tp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tL8G5eLByuu"
      },
      "outputs": [],
      "source": [
        "class_counts_test = np.sum(test_labs, axis=0)\n",
        "class_counts_pred = np.sum(binary_predictions, axis=0)\n",
        "\n",
        "for idx, (name, count_test, count_pred) in enumerate(zip(emotions, class_counts_test, class_counts_pred)):\n",
        "    print(f\"{name}: {count_test} ground truth, {count_pred} predictions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLWrULxLfucw"
      },
      "outputs": [],
      "source": [
        "# Convert multi-label to multi-class --> this is potentially wrong becouse it disregards fact of multi-label instances\n",
        "# true_classes = test_labs.argmax(axis=1)\n",
        "# predicted_classes = predictions_array.argmax(axis=1)\n",
        "\n",
        "\n",
        "\n",
        "# ths = 0.5\n",
        "# predicted_classes = (predictions_array >= ths).astype(int)\n",
        "true_classes = test_labs\n",
        "report = classification_report(true_classes, binary_predictions, target_names=emotions, zero_division=0)\n",
        "\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvaDjGvu6bTG"
      },
      "outputs": [],
      "source": [
        "## calculate f1-macro for this augmented classes\n",
        "report_dict = classification_report(true_classes, binary_predictions, target_names=emotions, zero_division=0, output_dict=True)\n",
        "\n",
        "aug_clss = ['embarrassment','grief','nervousness','pride','relief']\n",
        "f1_aug_cls_scores = []\n",
        "\n",
        "for cls in aug_clss:\n",
        "  f1_aug_cls_score = report_dict[cls]['f1-score']\n",
        "  f1_aug_cls_scores.append(f1_aug_cls_score)\n",
        "\n",
        "average_f1_aug_cls = sum(f1_aug_cls_scores)/len(f1_aug_cls_scores)\n",
        "print(f\"Average F1-Score for augmented classes: {average_f1_aug_cls}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nn4lip4fluRP"
      },
      "source": [
        "# Plot and save results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zKxdv59gaDF"
      },
      "source": [
        "##**RESULTS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pn9UaI_UsJdr"
      },
      "outputs": [],
      "source": [
        "# model_experiment_name = f'modelX_experimentX'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zHwl-3eEJG3"
      },
      "outputs": [],
      "source": [
        "##save results to csv\n",
        "\n",
        "#save cm\n",
        "flatten_cm = cm.reshape(len(emotions),4)\n",
        "cm_df = pd.DataFrame(flatten_cm, index=emotions,columns=['True Negative', 'False Positive', 'False Negative', 'True Positive'])\n",
        "cm_df.to_csv(f'{path}{predicted_model_folder}performance_results/confusion_matrix.csv')\n",
        "\n",
        "#save report\n",
        "report = classification_report(true_classes, binary_predictions, target_names=emotions, zero_division=0, output_dict=True)\n",
        "report_df = pd.DataFrame(report).transpose()\n",
        "report_df = report_df.round(4)\n",
        "report_df.to_csv(f'{path}{predicted_model_folder}performance_results/classification_report.csv')\n",
        "\n",
        "#save rest results\n",
        "results =  {\n",
        "\"Metric\": [\"Number of Test Instances\", \"Hamming Loss\", \"F1 Macro\", \"F1 Micro\", \"Average F1 Augmented Classes\"],\n",
        "\"Value\": [len(test_df), hamming, f1_macro, f1_micro, average_f1_aug_cls]\n",
        "}\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df = results_df.round(4)\n",
        "results_df.to_csv(f'{path}{predicted_model_folder}performance_results/performance_results.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXP_yZnGgX4s"
      },
      "source": [
        "##**PLOTS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUGbND4fJK0Q"
      },
      "source": [
        "###Heatmap for CM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXAWyWHBltpA"
      },
      "outputs": [],
      "source": [
        "#HeatMaps for CM each label\n",
        "for i, c in enumerate(cm):\n",
        "    cm_df = pd.DataFrame(c, index=['True Negative', 'True Positive'],\n",
        "                             columns=['Predicted Negative', 'Predicted Positive'])\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm_df, annot=True, fmt='g', cmap='Reds')\n",
        "    plt.title(f'Confusion Matrix for {emotions[i]}')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.savefig(f'{path}{predicted_model_folder}performance_results/plots/cm/confusion_matrix_{emotions[i]}.png')\n",
        "\n",
        "    plt.clf()\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9DLMoMNJNaH"
      },
      "source": [
        "###PR curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfsMcWYU6K2-"
      },
      "outputs": [],
      "source": [
        "## PR curve for each label\n",
        "\n",
        "probabilities_np = probabilities.numpy()\n",
        "labels_np = labels.numpy()\n",
        "\n",
        "for i, emotion in enumerate(emotions):\n",
        "    precision, recall, threshold = precision_recall_curve(labels_np[:, i], probabilities_np[:, i])\n",
        "    plt.figure()\n",
        "    plt.step(recall, precision, where='post')\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.title(f'Precision-Recall Curve for {emotions[i]}')\n",
        "    # plt.savefig(f'{path}{predicted_model_folder}performance_results/plots/PR_curve/PR_curve_{emotions[i]}.png')\n",
        "    # plt.clf()\n",
        "    # plt.close()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7K6oIhy9qls"
      },
      "outputs": [],
      "source": [
        "## PR curve -- for all\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "for i, emotion in enumerate(emotions):\n",
        "    precision, recall, threshold = precision_recall_curve(labels_np[:, i], probabilities_np[:, i])\n",
        "    plt.step(recall, precision, where='post', label=f'{emotion}')\n",
        "\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.title('Precision-Recall Curve for All Labels')\n",
        "plt.legend(loc=\"upper right\")\n",
        "# plt.savefig(f'{path}{predicted_model_folder}performance_results/plots/PR_curve/PR_Curves_All.png')\n",
        "plt.clf()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSlPeCeu95Gq"
      },
      "outputs": [],
      "source": [
        "## PR curve -- for 5 aug classes\n",
        "aug_classes=['embarrassment', 'grief', 'nervousness','pride','relief',]\n",
        "plt.figure(figsize=(10, 8))  # Create a larger figure for clarity\n",
        "\n",
        "for emotion in aug_classes:\n",
        "    if emotion in emotions:\n",
        "        i = emotions.index(emotion)\n",
        "        precision, recall, threshold = precision_recall_curve(labels_np[:, i], probabilities_np[:, i])\n",
        "        plt.step(recall, precision, where='post', label=f'{emotion}')\n",
        "\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.title('Precision-Recall Curve for Selected Labels')\n",
        "plt.legend(loc=\"upper right\")\n",
        "# plt.savefig(f'{path}{predicted_model_folder}performance_results/plots/PR_curve/PR_Curves_Selected.png')\n",
        "plt.clf()\n",
        "plt.close()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwIx15fFJO3E"
      },
      "source": [
        "###ROC AUC curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyOJnaarZ3GE"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
        "\n",
        "#ROC curve and AUC -- for each label\n",
        "for i in range(28):\n",
        "    # fpr[i], tpr[i], _ = roc_curve(true_classes[:, i], predictions_array[:, i])\n",
        "    # roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "    fpr, tpr, _ = roc_curve(true_classes[:, i], probabilities[:, i])\n",
        "\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, color='darkred', lw=2, label=f'ROC curve (area = {roc_auc:.4f})')\n",
        "    plt.plot([0, 1], [0, 1], color='darkblue', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('FP Rate')\n",
        "    plt.ylabel('TP Rate')\n",
        "    plt.title(f'ROC Curve for {emotions[i]}')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.fill_between(fpr, tpr, color='burlywood', alpha=0.2)\n",
        "    # plt.show()\n",
        "    plt.savefig(f'{path}{predicted_model_folder}performance_results/plots/ROC_curve_AUC/ROC_AUC_{emotions[i]}.png')\n",
        "    plt.clf()\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYYwTH_LlYZE"
      },
      "outputs": [],
      "source": [
        "from matplotlib import cm\n",
        "#ROC curve and AUC -- for all\n",
        "\n",
        "auc_colormaps = {\n",
        "    'excellent': (cm.Reds, 0.94, 1.0),\n",
        "    'good': (cm.Blues, 0.92, 0.94),\n",
        "    'fair': (cm.Greens, 0.85, 0.92),\n",
        "    'poor': (cm.copper, 0.5, 0.85)\n",
        "}\n",
        "\n",
        "def get_color(auc_value):\n",
        "    for colormap, range_start, range_end in auc_colormaps.values():\n",
        "        if range_start <= auc_value <= range_end:\n",
        "            gradient_index = (auc_value - range_start) / (range_end - range_start)\n",
        "            return colormap(gradient_index)\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "\n",
        "for i in range(28):\n",
        "    fpr, tpr, _ = roc_curve(true_classes[:, i], probabilities[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    color = get_color(roc_auc)\n",
        "\n",
        "    for j in range(len(fpr) - 1):\n",
        "        plt.plot(fpr[j:j+2], tpr[j:j+2], color=color, lw=2)\n",
        "\n",
        "    plt.plot(fpr[-2:], tpr[-2:], color=color, lw=2, label=f'{emotions[i]} (AUC = {roc_auc:.4f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.title('ROC Curves for All Emotions')\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.savefig(f'{path}{predicted_model_folder}performance_results/plots/ROC_curve_AUC/ROC_AUC_all.png')\n",
        "plt.clf()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8T8t3oh4aDfk"
      },
      "outputs": [],
      "source": [
        "#ROC curve and AUC -- for 5 augmented label\n",
        "aug_classes=['embarrassment', 'grief', 'nervousness','pride','relief',]\n",
        "aug_classes_list= [emotions.index(emotion) for emotion in aug_classes if emotion in emotions]\n",
        "plt.figure(figsize=(16, 8))\n",
        "\n",
        "for i in aug_classes_list:\n",
        "  fpr, tpr, _ = roc_curve(true_classes[:, i], probabilities[:, i])\n",
        "  roc_auc = auc(fpr, tpr)\n",
        "  plt.plot(fpr, tpr, lw=2, label=f'{emotions[i]} (area = {roc_auc:.4f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('FP Rate')\n",
        "plt.ylabel('TP Rate')\n",
        "plt.title('ROC Curves for Aug Emotions')\n",
        "plt.legend(loc=\"upper right\")\n",
        "\n",
        "# plt.show()\n",
        "plt.savefig(f'{path}{predicted_model_folder}performance_results/plots/ROC_curve_AUC/ROC_AUC_5augcls.png')\n",
        "plt.clf()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcH9IsFtJRD6"
      },
      "source": [
        "###F1 by label plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0Xfq4rQgj1u"
      },
      "outputs": [],
      "source": [
        "##F1 Score by Label bar chart\n",
        "report = classification_report(true_classes, binary_predictions, target_names=emotions, output_dict=True, zero_division=0)\n",
        "f1_all_cls = [report[emotion]['f1-score'] for emotion in emotions]\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(14,10))\n",
        "plt.bar(emotions, f1_all_cls, color='darkred')\n",
        "plt.xlabel('Emotions')\n",
        "plt.ylabel('F1 Score')\n",
        "plt.title('F1 Scores by Label')\n",
        "plt.xticks(rotation=90)\n",
        "plt.ylim([0, 1])\n",
        "plt.savefig(f'{path}{predicted_model_folder}performance_results/plots/f1_all_barchart.png')\n",
        "plt.clf()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nw5iAJALJUcB"
      },
      "source": [
        "###Density prob. plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LC-TKc5uf_gN"
      },
      "outputs": [],
      "source": [
        "##density plots of prob for each class\n",
        "\n",
        "for i in range(28):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.kdeplot(probabilities[:, i], fill=True)\n",
        "    plt.title(f'Density Plot of Predictions for {emotions[i]}')\n",
        "    plt.xlabel('Prediction Probability')\n",
        "    plt.ylabel('Density')\n",
        "    plt.legend(loc=\"upper right\")\n",
        "    plt.savefig(f'{path}/{predicted_model_folder}performance_results/plots/density_plots/density_plot{emotions[i]}.png')\n",
        "    plt.clf()\n",
        "    plt.close()\n",
        "\n",
        "# Plotting density plots for all classes on the same plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "for i in range(probabilities.shape[1]):\n",
        "    sns.kdeplot(probabilities[:, i], label=emotions[i])\n",
        "\n",
        "plt.title('Density Plot of Predictions for All Labels')\n",
        "plt.xlabel('Prediction Probability')\n",
        "plt.ylabel('Density')\n",
        "plt.legend(title='Emotions',loc=\"upper right\")\n",
        "plt.savefig(f'{path}{predicted_model_folder}performance_results/plots/density_plots/density_plot_all.png')\n",
        "plt.clf()\n",
        "plt.close()\n",
        "\n",
        "\n",
        "##density plot for 5 aug cls\n",
        "aug_classes=['embarrassment', 'grief', 'nervousness','pride','relief',]\n",
        "cls_map = [emotions.index(cls) for cls in aug_classes]\n",
        "plt.figure(figsize=(10, 6))\n",
        "for i in cls_map:\n",
        "      sns.kdeplot(probabilities[:, i], label=emotions[i], fill=True)\n",
        "plt.title('Density Plot of Predictions for Selected Emotions')\n",
        "plt.xlabel('Prediction Probability')\n",
        "plt.ylabel('Density')\n",
        "plt.xlim(-0.1, 0.1)\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.savefig(f'{path}{predicted_model_folder}performance_results/plots/density_plots/density_plot_5AugCls.png')\n",
        "plt.clf()\n",
        "plt.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C43Kc1uUq9yt"
      },
      "source": [
        "###Label frequency distribution plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEZHTeSbSunL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "len_train_before = None\n",
        "# ##Label frequency distribution -- in training dataset and in numbers of predicted in each class\n",
        "train_file_path = f'{path}/{predicted_model_folder}/train.tsv'\n",
        "training_dataset_file_path = f'{path}{predicted_model_folder}/training_dataset.tsv'\n",
        "train = pd.read_csv(train_file_path, sep='\\t', header=None)\n",
        "\n",
        "if os.path.exists(training_dataset_file_path):\n",
        "    training_dataset = pd.read_csv(training_dataset_file_path, sep='\\t', header=None)\n",
        "    len_train_before = len(training_dataset)\n",
        "    training_dataset = pd.concat([train, training_dataset], ignore_index=True)\n",
        "else:\n",
        "    training_dataset = train\n",
        "\n",
        "len_train_after = len(training_dataset)\n",
        "\n",
        "with open(f'{path}{predicted_model_folder}performance_results/length.txt', 'w') as file:\n",
        "  file.write(f'Length of enhancing data: {len_train_before}\\n')\n",
        "  file.write(f'Length of train dataset after enhancing: {len_train_after}\\n')\n",
        "\n",
        "\n",
        "train_X_df, train_labels_df = training_dataset[0], training_dataset[1]\n",
        "single_labels_df = train_labels_df[train_labels_df.apply(lambda x: str(x).isdigit())]\n",
        "single_labels_df = single_labels_df.astype(int)\n",
        "class_distribution = single_labels_df.value_counts()\n",
        "class_distribution = class_distribution[class_distribution.index != 27].sort_index()\n",
        "print(class_distribution.sort_index())\n",
        "\n",
        "emotion_names = emotions[:-1]\n",
        "\n",
        "plt.figure(figsize=(16, 12))\n",
        "class_distribution.plot(kind='bar', color='darkred')\n",
        "plt.title('Class Distribution in Training Dataset')\n",
        "plt.xlabel('Class Label')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(ticks=range(len(emotion_names)), labels=emotion_names, rotation=90)\n",
        "plt.grid(axis='y')\n",
        "plt.savefig(f'{path}{predicted_model_folder}performance_results/plots/class_distribution.png')\n",
        "plt.clf()\n",
        "plt.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYv6w1JeTZ7J"
      },
      "source": [
        "###Plot count predicted vs test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVSx3itKspgf"
      },
      "outputs": [],
      "source": [
        "## number in test set each label vs number of predicted each label -- grouped bar cha\n",
        "##withou neutral as it has many instances and doesnt need for analysis\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(20, 12))\n",
        "\n",
        "index = np.arange(len(emotions[:-1]))\n",
        "bar_width = 0.4\n",
        "\n",
        "bar1 = ax.bar(index, class_counts_test[:-1], bar_width, label='Test Set', color = \"blue\", edgecolor = \"black\")\n",
        "bar2 = ax.bar(index + bar_width, class_counts_pred[:-1], bar_width, label='Predictions',color = \"red\",\n",
        "       edgecolor = \"black\")\n",
        "\n",
        "ax.set_xlabel('Emotions')\n",
        "ax.set_ylabel('Counts')\n",
        "ax.set_title('Counts by Emotion and Set')\n",
        "ax.set_xticks(index + bar_width / 2)\n",
        "ax.set_xticklabels(emotions[:-1],rotation = 90)\n",
        "ax.legend()\n",
        "plt.savefig(f'{path}{predicted_model_folder}performance_results/plots/count_test_pred.png')\n",
        "plt.clf()\n",
        "plt.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "1gYgZPqLV0Ck",
        "R8Btvk5vVgfB",
        "Nn4lip4fluRP",
        "3zKxdv59gaDF",
        "ZXP_yZnGgX4s",
        "PUGbND4fJK0Q",
        "B9DLMoMNJNaH",
        "FwIx15fFJO3E",
        "TcH9IsFtJRD6",
        "Nw5iAJALJUcB",
        "C43Kc1uUq9yt",
        "uXUhmJpUrCrC"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}